{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOR 320 Intro to Data Science - CART\n",
    "\n",
    "This lecture is comprised of two main sections:\n",
    "\n",
    "- 1. Classification Trees\n",
    "\n",
    "- 2. Regression Trees\n",
    "\n",
    "The lecture we will dive deeper into the `sklearn` package to make use of algorithms such as Classification Trees (`DecisionTreeClassifier`) and Regression Trees (`DecisionTreeRegressor`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note: How to hide the python code and cell index in jupyter notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "$(document).ready(function() {\n",
       "    // Show input code cells\n",
       "    $('div.input').show(); \n",
       "    \n",
       "    // Show output prompts (\"In []\" and \"Out []\")\n",
       "    $('.prompt').show();\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''\n",
    "<script>\n",
    "$(document).ready(function() {\n",
    "    // Show input code cells\n",
    "    $('div.input').show(); \n",
    "    \n",
    "    // Show output prompts (\"In []\" and \"Out []\")\n",
    "    $('.prompt').show();\n",
    "});\n",
    "</script>\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "$(document).ready(function() {\n",
       "    // Hide input code cells\n",
       "    $('div.input').hide(); \n",
       "    \n",
       "    // Hide output prompts (\"Out []\")\n",
       "    $('.prompt').hide();\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''\n",
    "<script>\n",
    "$(document).ready(function() {\n",
    "    // Hide input code cells\n",
    "    $('div.input').hide(); \n",
    "    \n",
    "    // Hide output prompts (\"Out []\")\n",
    "    $('.prompt').hide();\n",
    "});\n",
    "</script>\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''\n",
    "<script>\n",
    "$(document).ready(function() {\n",
    "    // Show input code cells\n",
    "    $('div.input').show(); \n",
    "    \n",
    "    // Show output prompts (\"In []\" and \"Out []\")\n",
    "    $('.prompt').show();\n",
    "});\n",
    "</script>\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parole = pd.read_csv(\"NYCparole.csv\")\n",
    "parole.info()\n",
    "parole.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CLASSIFICATION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is CART an appropriate model for classification with Parole data? \n",
    "\n",
    "https://datascience.stackexchange.com/questions/6048/decision-tree-or-logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting the response variable as Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parole['Violator'] = parole['Violator'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous labs with `statsmodels`, we left the handling of categorical variables to the model.\n",
    "With `sklearn` packages, we need to do the categorical encoding ourselves.\n",
    "\n",
    "\n",
    "Two widely used types of categorical encoding are \"Dummy Encoding\" and \"One-hot encoding\":\n",
    "- One-hot encoding converts a variable that has n possible distinct values, into n-1 binary variables. \n",
    "- Dummy encoding converts a variable with n possible distinct values, into n binary variables.  \n",
    "\n",
    "For regression-type problems One-hot Encoding is preferable as we wish to avoid perfect collinearity between the binary variables that are being generated by the encoding. In the classification problem however, we are not concerned about inflating factors and hence can choose either of the two schemes. They will produce equivalent encodings.\n",
    "\n",
    "We will apply the encoding scheme to the `Class` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parole_enc = pd.get_dummies(parole, columns = ['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.get_dummies()` can also automatically detect the categorical columns if no specific columns are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parole_enc.info()\n",
    "parole_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = parole_enc['Violator']\n",
    "X = parole_enc.drop(['Violator'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,  \n",
    "                                                    stratify=parole_enc['Violator'],\n",
    "                                                    random_state=88,)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BASELINE\n",
    "\n",
    "negative = np.sum(y_train == 0)\n",
    "positive = np.sum(y_train == 1)\n",
    "print(pd.Series({'0': negative, '1': positive}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "\n",
    "`skelarn`'s `DecisionTreeClassifier` is just one of many implementations that we can use for our classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(min_samples_leaf=5, \n",
    "                             ccp_alpha=0.001,\n",
    "                             random_state = 88)\n",
    "\n",
    "dtc = dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "print('Node count =', dtc.tree_.node_count)\n",
    "plt.figure(figsize=(12,12))\n",
    "plot_tree(dtc, \n",
    "          feature_names=X_train.columns.tolist(), \n",
    "          class_names=['0','1'], \n",
    "          filled=True,\n",
    "          impurity=True,\n",
    "          rounded=True,\n",
    "          fontsize=12) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try increasing the complexity parameter to prune the tree\n",
    "\n",
    "dtc2 = DecisionTreeClassifier(min_samples_leaf=5, \n",
    "                              ccp_alpha=0.005,\n",
    "                              random_state = 88)\n",
    "\n",
    "dtc2 = dtc2.fit(X_train, y_train)\n",
    "\n",
    "print('Node count =', dtc2.tree_.node_count)\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_tree(dtc2, \n",
    "          feature_names=X_train.columns.tolist(), \n",
    "          class_names=['0','1'], \n",
    "          filled=True,\n",
    "          impurity=True,\n",
    "          rounded=True,\n",
    "          fontsize=12)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAKE PREDICTIONS \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_pred = dtc2.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print (\"Confusion Matrix : \\n\", cm)\n",
    "print('Precision:',precision_score(y_test, y_pred))\n",
    "print('Recall:',recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that our tree is not able to discern very well between positive cases of `Violator` and negative cases of `Violator`. The highly imbalanced dataset is likely to be one of the reasons why our predictions are heavily tilted towards `Violator = 0`. We will address the class imbalance with the help of a custom loss function. More specifically, we will assign individual weights to each of the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Losses and Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would like to produce a loss function that can reflect specific penalties for the various types of classification choices.\n",
    "\n",
    "- cost_TP = 0\n",
    "- cost_TN = 0\n",
    "- cost_FP = 1\n",
    "- cost_FN = 20\n",
    "\n",
    "With the `sklearn`'s implementation of `DecisionTreeClassifier`, we are not able to incorporate directly such a function. However, we can achieve the same result by assigning weights to the classes in a way that it mimics the presence of more observations of the minority class. This will accomplish the goal of rebalancing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtc3 = DecisionTreeClassifier(min_samples_leaf=5, \n",
    "                              ccp_alpha=0.007,\n",
    "                              class_weight = {0: 1, 1: 20},\n",
    "                              random_state = 88)\n",
    "\n",
    "dtc3 = dtc3.fit(X_train, y_train)\n",
    "\n",
    "print('Node count =', dtc3.tree_.node_count)\n",
    "plt.figure(figsize=(11,9))\n",
    "plot_tree(dtc3, \n",
    "          feature_names=X_train.columns.tolist(), \n",
    "          class_names=['0','1'], \n",
    "          filled=True,\n",
    "          impurity=False,\n",
    "          rounded=True,\n",
    "          fontsize=12) \n",
    "plt.show()\n",
    "\n",
    "y_pred = dtc3.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print (\"Confusion Matrix : \\n\", cm) \n",
    "print('Precision:',precision_score(y_test, y_pred))\n",
    "print('Recall:',recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. REGRESSION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Regression Tree example, we use our old wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"wine_agg.csv\")\n",
    "wine.head()\n",
    "len(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_train = wine[wine['Year'] <= 1985]\n",
    "wine_test = wine[wine['Year'] > 1985]\n",
    "\n",
    "y_train = wine_train['LogAuctionIndex']\n",
    "X_train = wine_train.drop(['LogAuctionIndex'], axis=1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(min_samples_split=10, \n",
    "                            ccp_alpha=0.02,\n",
    "                            random_state = 88)\n",
    "dtr = dtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Node count =', dtr.tree_.node_count)\n",
    "plt.figure(figsize=(9,7))\n",
    "plot_tree(dtr, \n",
    "          feature_names=X_train.columns.tolist(), \n",
    "          class_names=['0','1'], \n",
    "          filled=True,\n",
    "          impurity=False,\n",
    "          rounded=True,\n",
    "          fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def OSR2(model, X_test, y_test, y_train):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    SSE = np.sum((y_test - y_pred)**2)\n",
    "    SST = np.sum((y_test - np.mean(y_train))**2)\n",
    "                 \n",
    "    return (1 - SSE/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test = wine_test['LogAuctionIndex']\n",
    "X_test = wine_test.drop(['LogAuctionIndex'], axis=1)\n",
    "\n",
    "print('OSR2:', OSR2(dtr, X_test, y_test, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A negative OSR2 is an indicator that the model doesn't work well on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
